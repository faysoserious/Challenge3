{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "filenames = glob.glob('C:\\\\Users\\\\fxie_\\\\Google Drive\\\\02807 Computational Tools for Big Data\\\\video\\\\*.mp4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from moviepy.editor import *\n",
    "import numpy as np\n",
    "import os\n",
    "import itertools\n",
    "import PIL\n",
    "from dHash import DHash\n",
    "import time\n",
    "\n",
    "def extract_frames(filename):\n",
    "    head, tail = os.path.split(filename)\n",
    "#    print('read video {}.mp4'.format(tail.split('.')[0]))\n",
    "    clip = VideoFileClip(filename)\n",
    "    #resize each frame reduce the computation cost\n",
    "    resized = clip.resize((100,100))\n",
    "    frames_list = [x for x in resized.iter_frames()]\n",
    "#   merge all the extracted frames as a long image\n",
    "    frames = np.stack(frames_list, axis=0)\n",
    "#    frames = np.vstack(frames)\n",
    "#    frames = frames_list[len(frames_list)//2]\n",
    "    feature = dict()\n",
    "    val = list()\n",
    "    for img in(frames):\n",
    "        feature[DHash.calculate_hash(PIL.Image.fromarray(np.uint8(img)))] = img  \n",
    "    combination = list( itertools.combinations(list(feature.keys()), 2) )\n",
    "    diff = list()\n",
    "    for a,b in combination:\n",
    "        diff.append(DHash.hamming_distance(a,b))\n",
    "    key1,key2 = combination[diff.index(max(diff))]\n",
    "    keyfeature = np.hstack((feature[key1],feature[key2]))\n",
    "    return keyfeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.Hashing for each video\n",
      "1 videos has been read\n",
      "11 videos has been read\n",
      "21 videos has been read\n",
      "31 videos has been read\n",
      "41 videos has been read\n",
      "51 videos has been read\n",
      "61 videos has been read\n",
      "71 videos has been read\n",
      "81 videos has been read\n",
      "91 videos has been read\n",
      "101 videos has been read\n",
      "111 videos has been read\n",
      "121 videos has been read\n",
      "131 videos has been read\n",
      "141 videos has been read\n",
      "done in 90.111s.\n",
      "2.Clustering all the hash\n",
      "done in 93.201s.\n",
      "done in 93.217s.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "print(\"1.Hashing for each video\")\n",
    "\n",
    "t0 = time.time()\n",
    "my_dict = dict()\n",
    "for index,file in enumerate(filenames):\n",
    "    head, tail = os.path.split(file)\n",
    "    my_dict[tail.split('.')[0]]=DHash.calculate_hash(PIL.Image.fromarray(np.uint8(extract_frames(file))))\n",
    "    if(index%10==0):\n",
    "        print(\"{} videos has been read\".format(index+1))\n",
    "print(\"done in %0.3fs.\" % (time.time() - t0))    \n",
    "\n",
    "print(\"2.Clustering all the hash\")\n",
    "n_video = 10\n",
    "comparison = list( itertools.product(list(my_dict.values()), repeat =2) ) \n",
    "cluster_matrix = dict()\n",
    "for key1,key2 in comparison:\n",
    "    cluster_matrix[(key1,key2)]=DHash.hamming_distance(key1,key2)\n",
    "buckets = np.array(list(cluster_matrix.values()))\n",
    "hash_sample = buckets.reshape(len(filenames),len(filenames))\n",
    "kmeans = KMeans(n_clusters=n_video, random_state=0,n_jobs=-1).fit_predict(hash_sample)\n",
    "\n",
    "clusters =dict()\n",
    "for i in range(n_video):\n",
    "    clusters[i] = []\n",
    "print(\"done in %0.3fs.\" % (time.time() - t0))    \n",
    "    \n",
    "\n",
    "for index, value in enumerate(kmeans):\n",
    "    head, tail = os.path.split(filenames[index])\n",
    "    clusters[value].append(tail.split('.')[0])\n",
    "print(\"done in %0.3fs.\" % (time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
